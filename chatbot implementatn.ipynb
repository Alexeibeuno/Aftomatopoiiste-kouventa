{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395aaf42",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89cabe",
   "metadata": {},
   "source": [
    "### import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba36776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3806ecd",
   "metadata": {},
   "source": [
    "### importing and reading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8613046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = open('dialogs.txt','r')\n",
    "chat_doc = chat.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea3b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the entire text into uppercase or lowercase, so that the algorithm does not treat the same words in different cases as different\n",
    "\n",
    "chat_doc = chat_doc.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb552b6",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "\n",
    "##### Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc821ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adhua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ae5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adhua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f669fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to list of sentences \n",
    "sent_tokens = nltk.sent_tokenize(chat_doc)\n",
    "\n",
    "# converts to list of words\n",
    "word_tokens = nltk.word_tokenize(chat_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590fc25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi, how are you doing?', \"i'm fine.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3a40b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415ab45",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224c120",
   "metadata": {},
   "source": [
    "##### Lemmatization: A slight variant of stemming is lemmatization. \n",
    "##### The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c49e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer() #to initialize Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "774b0676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greet'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer.lemmatize('greetings', pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a4d2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmertokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return Lemmertokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99744787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'are', 'you', 'doing']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LemNormalize('how are you doing?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10e038d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword matching\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b23ef5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am glad! You are talking to me'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeting('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6aa4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
